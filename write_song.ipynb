{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc821a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N gram with Pytorch\n",
    "import torch \n",
    "from torch import nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "940769e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, out_features)  # Define the linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear1(x)  # Apply the linear layer in the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a283d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=3\n",
    "context_size=2\n",
    "vocab_size=4\n",
    "\n",
    "embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "linear = nn.Linear(context_size * embedding_dim, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "034026d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c81bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embedding = embeddings(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d06bc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embedding_c = torch.reshape( my_embedding, (-1, context_size * embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3d19221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0150, 0.3244]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(my_embedding_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "324aa6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # <-- ADD THIS\n",
    "\n",
    "class NgramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NgramLanguageModeler, self).__init__()\n",
    "\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds = torch.reshape(embeds, (-1, self.context_size * self.embedding_dim))\n",
    "\n",
    "        out = F.relu(self.linear1(embeds))  # <-- FIXED LAYER NAME\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc74046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29ac8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "BATCH_SIZE = 10 \n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "def collate_batch(batch):\n",
    "    batch_size = len(batch)\n",
    "    context, target = [], []\n",
    "\n",
    "    for i in range(CONTEXT_SIZE, batch_size):\n",
    "        target.append(vocab([batch[i]]))\n",
    "        context.append(vocab([batch[i-j-1] for j in range(CONTEXT_SIZE)]))\n",
    "        return torch.tensor(context).to(device), torch.tensor(target).to(device.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2429ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['never', 'gonna', 'break', 'your', 'loop', 'ml', 'parody', 'to', 'the', 'tune', 'of', 'never', 'gonna', 'give', 'you', 'up', 'we', 'are', 'no', 'strangers', 'to', 'code', 'you', 'train', 'the', 'weights', 'and', 'i', 'watch', 'it', 'explode', 'backprop', \"'\", 's', 'running', 'through', 'the', 'night', 'tuning', 'those', 'layers', \"'\", 'til', 'the', 'loss', 'feels', 'right', 'and', 'if', 'you', 'ask', 'me', 'how', 'it', 'works', 'don’t', 'you', 'worry', 'i’ll', 'just', 'say', 'it’s', 'gradients', 'and', 'matrix', 'math', 'with', 'tensors', 'dancing', 'every', 'way', 'never', 'gonna', 'break', 'your', 'loop', 'never', 'gonna', 'drop', 'your', 'batch', 'never', 'gonna', 'run', 'out', 'of', 'ram', 'and', 'crash', 'you', 'never', 'gonna', 'overfit', 'never', 'gonna', 'undershoot', 'never', 'gonna', 'tell', 'a', 'model', 'lie', 'and', 'bash', 'you']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Choose tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Text to tokenize\n",
    "song = (\n",
    "    \"Never gonna break your loop ml parody \"\n",
    "    \"to the tune of never gonna give you up \"\n",
    "    \"we are no strangers to code \"\n",
    "    \"you train the weights and I watch it explode \"\n",
    "    \"backprop's running through the night \"\n",
    "    \"tuning those layers 'til the loss feels right \"\n",
    "    \"and if you ask me how it works \"\n",
    "    \"don’t you worry I’ll just say \"\n",
    "    \"it’s gradients and matrix math \"\n",
    "    \"with tensors dancing every way \"\n",
    "    \"never gonna break your loop \"\n",
    "    \"never gonna drop your batch \"\n",
    "    \"never gonna run out of RAM and crash you \"\n",
    "    \"never gonna overfit \"\n",
    "    \"never gonna undershoot \"\n",
    "    \"never gonna tell a model lie and bash you\"\n",
    ")\n",
    "\n",
    "# Apply tokenizer to the song\n",
    "tokens = tokenizer(song)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19ac773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'gonna', 'never', 'you', 'and', 'the', 'your', \"'\", 'break', 'it', 'loop', 'of', 'to', 'a', 'are', 'ask', 'backprop', 'bash', 'batch', 'code']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Song is a single long string — this is fine\n",
    "song = (\n",
    "    \"Never gonna break your loop ml parody \"\n",
    "    \"to the tune of never gonna give you up \"\n",
    "    \"we are no strangers to code \"\n",
    "    \"you train the weights and I watch it explode \"\n",
    "    \"backprop's running through the night \"\n",
    "    \"tuning those layers 'til the loss feels right \"\n",
    "    \"and if you ask me how it works \"\n",
    "    \"don’t you worry I’ll just say \"\n",
    "    \"it’s gradients and matrix math \"\n",
    "    \"with tensors dancing every way \"\n",
    "    \"never gonna break your loop \"\n",
    "    \"never gonna drop your batch \"\n",
    "    \"never gonna run out of RAM and crash you \"\n",
    "    \"never gonna overfit \"\n",
    "    \"never gonna undershoot \"\n",
    "    \"never gonna tell a model lie and bash you\"\n",
    ")\n",
    "\n",
    "# ✅ Correct: pass the whole string to the tokenizer\n",
    "token_lists = [tokenizer(song)]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(token_lists, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Create pipeline\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "# Inspect vocab\n",
    "index_to_token = vocab.get_itos()\n",
    "print(index_to_token[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc558573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['never', 'going', 'to', 'give', 'you', 'up']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('never going to give you up'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83186628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "763d0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, number_of_epochs=100, show=10):\n",
    "    MY_LOSS = []\n",
    "    for epoch in tqdm(range(number_of_epochs)):\n",
    "        total_loss = 0 \n",
    "\n",
    "        for context, target in dataloader:\n",
    "            model.zero_grad()\n",
    "            predicted = model(context)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = criterion(predicted, target.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            MY_LOSS.append(total_loss/len(dataloader))\n",
    "    return MY_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d6ec4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "padding=BATCH_SIZE-len(tokens)%BATCH_SIZE\n",
    "\n",
    "tokens_pad=tokens+tokens[0:padding]\n",
    "dataloader = DataLoader(tokens_pad, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = NgramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ad57638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'gonna', 'never', 'you', 'and', 'the', 'your', \"'\", 'break', 'it', 'loop', 'of', 'to', 'a', 'are', 'ask', 'backprop', 'bash', 'batch', 'code', 'crash', 'dancing', 'don’t', 'drop', 'every', 'explode', 'feels', 'give', 'gradients', 'how', 'i', 'if', 'it’s', 'i’ll', 'just', 'layers', 'lie', 'loss', 'math', 'matrix', 'me', 'ml', 'model', 'night', 'no', 'out', 'overfit', 'parody', 'ram', 'right', 'run', 'running', 's', 'say', 'strangers', 'tell', 'tensors', 'those', 'through', 'til', 'train', 'tune', 'tuning', 'undershoot', 'up', 'watch', 'way', 'we', 'weights', 'with', 'works', 'worry']\n"
     ]
    }
   ],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "print(index_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0cbfbdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([69])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'with'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = text_pipeline(\"Never gonna\")\n",
    "\n",
    "\n",
    "x_c = torch.tensor(context).reshape(-1,1)\n",
    "\n",
    "out=model(x_c)\n",
    "predicted_index = torch.argmax(out,1)\n",
    "\n",
    "print(predicted_index)\n",
    "\n",
    "display(index_to_token[predicted_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0fb724f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_song(model, number_of_words=100):\n",
    "    my_song=\"\"\n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad():\n",
    "            context =torch.tensor(vocab([tokens[i-j-1-1] for j in range(CONTEXT_SIZE)])).to(device)\n",
    "            word_inx = torch.argmax(model(context))\n",
    "            my_song+=\" \"+index_to_token[word_inx.detach().item()]\n",
    "    return my_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d0d013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loop loop those s loop ask i’ll loop loop loop run with with with s those ask loop <unk> ask with s loop run watch layers give i’ll ask ask loop matrix those ask <unk> those drop those loop loop loop drop matrix feels loop give ask loop ask explode crash weights explode ask you i’ll layers break loop loop loop drop you run loop explode give loop loop loop ask <unk> bash give s loop ask i’ll ask s loop loop if every s <unk> run are loop explode break loop those s drop with s bash ask s\n"
     ]
    }
   ],
   "source": [
    "print(write_song(model, number_of_words=100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
