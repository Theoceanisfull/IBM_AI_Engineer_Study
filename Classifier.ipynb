{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8bc8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==2.0.1 torchtext==0.15.2 torchdata==0.6.1 --force-reinstall --no-cache-dir\n",
    "#!pip install portalocker>=2.0.0\n",
    "\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install \"portalocker>=2.0.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b0bbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==2.1.0 torchtext==0.14.1\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2c79016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = AG_NEWS(split='train')\n",
    "print(next(iter(train_iter)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19de53ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "device = torch.device (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b89441ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 3, Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Label 3, Text: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "Label 3, Text: Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "Label 3, Text: Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "Label 3, Text: Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "test_iter = AG_NEWS(split='test')\n",
    "for i, (label, line) in zip(range(5), train_iter):\n",
    "    print(f'Label {label}, Text: {line}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a739b5",
   "metadata": {},
   "source": [
    "ðŸ§± Step 4: Tokenize the Text and Build the Vocabulary\n",
    "ðŸ§  Whatâ€™s happening here?\n",
    "To prepare text for model training, we must:\n",
    "Tokenize the text (split into words).\n",
    "Build a vocabulary from all tokens (map words to integer indices).\n",
    "This is necessary so we can convert raw text â†’ tensors.\n",
    "Weâ€™ll use torchtext's built-in tokenizer and vocabulary builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bda49d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 95811\n",
      "Example tokens: ['u', '.', 's', '.', 'stock', 'rally', 'to', 'new', 'highs', 'after', 'trade', 'deals']\n",
      "Token indices: [51, 1, 9, 1, 294, 688, 4, 23, 1723, 34, 268, 1809]\n"
     ]
    }
   ],
   "source": [
    "#Setup tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "#Build vocab from training dataset\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(f'Vocab Size: {len(vocab)}')\n",
    "print(f\"Example tokens: {tokenizer('U.S. Stock rally to new highs after trade deals')}\")\n",
    "print(f\"Token indices: {[vocab[token] for token in tokenizer('U.S. Stock rally to new highs after trade deals')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2510351",
   "metadata": {},
   "source": [
    "ðŸ”¢ Step 5: Encode Text and Labels into Tensors\n",
    "ðŸ§  Why this matters:\n",
    "Before passing data into a model, we must:\n",
    "Convert each text sample into a list of integer token indices.\n",
    "Convert each label into an integer (already done in AG_NEWS, but can be mapped manually for custom data).\n",
    "Wrap this logic in a function for DataLoader use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce15236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: U.S. Stock rally to new highs after trade deals\n",
      "Token indices: [51, 1, 9, 1, 294, 688, 4, 23, 1723, 34, 268, 1809]\n",
      "Label index: 2\n"
     ]
    }
   ],
   "source": [
    "# Converts a raw string into a list of token indices using vocab\n",
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "# Labels in AG_NEWS are already 1-based: [1, 2, 3, 4]\n",
    "# We'll subtract 1 to make them 0-based: [0, 1, 2, 3]\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1\n",
    "\n",
    "# Test it out\n",
    "sample_text = \"U.S. Stock rally to new highs after trade deals\"\n",
    "sample_label = 3  # Sci/Tech (just an example)\n",
    "\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Token indices: {text_pipeline(sample_text)}\")\n",
    "print(f\"Label index: {label_pipeline(sample_label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0ec2ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', '.', 's', '.', 'stock', 'rally', 'to', 'new', 'highs', 'after', 'trade', 'deals']\n",
      "Tokens: ['u', '.', 's', '.', 'stock', 'rally', 'to', 'new', 'highs', 'after', 'trade', 'deals']\n",
      "Indices: [51, 1, 9, 1, 294, 688, 4, 23, 1723, 34, 268, 1809]\n",
      "Length match? True\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"U.S. Stock rally to new highs after trade deals\"\n",
    "tokens = tokenizer(sample_text)\n",
    "print(tokens)\n",
    "\n",
    "[token for token in tokens if token not in vocab.get_stoi()]\n",
    "\n",
    "[vocab[token] for token in tokens]\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "tokens = tokenizer(sample_text)\n",
    "indices = text_pipeline(sample_text)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Indices: {indices}\")\n",
    "print(f\"Length match? {len(tokens) == len(indices)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda45fe3",
   "metadata": {},
   "source": [
    "ðŸ§° Step 6: Define Collate Function and Set Up DataLoader\n",
    "ðŸ§  Why this step matters:\n",
    "PyTorch's DataLoader batches raw data.\n",
    "But with text data, each sample has a different length.\n",
    "So we need a custom collate function to:\n",
    "Apply token and label pipelines\n",
    "Pad or merge sequences\n",
    "Return batch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89d22669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Collate function to process a batch of (label, text) tuples\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    for label, text in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)  # Running total offsets\n",
    "    text_tensor = torch.cat(text_list)\n",
    "    \n",
    "    return label_tensor.to(device), text_tensor.to(device), offsets.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2da13a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassificationModel, self).__intit__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b34f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab)\n\u001b[1;32m      2\u001b[0m embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m           \u001b[38;5;66;03m# You can use 100, 128, etc. if needed\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]))  \u001b[38;5;66;03m# or 2 for binary classification\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m TextClassificationModel(vocab_size, embed_dim, num_classes)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 64           # You can use 100, 128, etc. if needed\n",
    "num_classes = len(set(['label']))  # or 2 for binary classification\n",
    "\n",
    "model = TextClassificationModel(vocab_size, embed_dim, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f03ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, text, offsets):\n",
    "    embedded = self.embedding(text, offsets)\n",
    "    return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd0db0",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd9216b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for label, text, offsets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (output.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "    return total_acc / total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835ecef",
   "metadata": {},
   "source": [
    "Check how well it learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5690a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text, offsets in dataloader:\n",
    "            output = model(text, offsets)\n",
    "            total_acc += (output.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a000c7",
   "metadata": {},
   "source": [
    "Run the model for rounds (Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26399460",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m evaluate(test_iter)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Valiation Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(dataloader):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m     total_acc, total_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, text, offsets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train_acc = train_model(train_iter)\n",
    "    val_acc = evaluate(test_iter)\n",
    "    print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc:.4f}, Valiation Accuracy = {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
